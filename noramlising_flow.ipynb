{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fc15436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "DATASET_PATH = \"../data\"\n",
    "\n",
    "CHECKPOINT_PATH = \"../saved_models/data\"\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8578fec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2235e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial11/\"\n",
    "pretrained_files = [\"MNISTFlow_simple.ckpt\", \"MNISTFlow_vardeq.ckpt\", \"MNISTFlow_multiscale.ckpt\"]\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "for file_name in pretrained_files:\n",
    "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
    "    if not os.path.isfile(file_path):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\"Something went wrong\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c7e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(sample):\n",
    "    return (sample * 255).to(torch.int32)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                discretize])\n",
    "\n",
    "train_dataset = MNIST(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [50000, 10000])\n",
    "test_set = MNIST(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = data.DataLoader(train_set, batch_size=256, shuffle=False, drop_last=False)\n",
    "val_loader = data.DataLoader(val_set, batch_size=64, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_loader = data.DataLoader(test_set, batch_size=64, shuffle=False, drop_last=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29db205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_imgs(imgs, title=None, row_size=4):\n",
    "    num_imgs = imgs.shape[0] if isinstance(imgs, torch.Tensor) else len(imgs)\n",
    "    is_int = imgs.dtype==torch.int32 if isinstance(imgs, torch.Tensor) else imgs[0].dtype==torch.int32\n",
    "    nrow = min(num_imgs, row_size)\n",
    "    ncol = int(math.ceil(num_imgs/nrow))\n",
    "    imgs = torchvision.utils.make_grid(imgs, nrow=nrow, pad_value=128 if is_int else 0.5)\n",
    "    np_imgs = imgs.cpu().numpy()\n",
    "    plt.figure(figsize=(1.5*nrow, 1.5*ncol))\n",
    "    plt.imshow(np.transpose(np_imgs, (1,2,0)), interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "show_imgs([train_set[i][0] for i in range(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "528791bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError:\n",
    "    !pip install --quiet pytorch-lightning>=1.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d978d006",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFlow(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, flows, import_samples=8):\n",
    "        super().__init__()\n",
    "        self.flows = nn.ModuleList(flows)\n",
    "        self.import_samples = import_samples\n",
    "        #prior distribution for final latent space\n",
    "        self.prior = torch.distributions.normal.Normal(loc=0.0, scale=1.0)\n",
    "        self.example_input_array = train_set[0][0].unsqueeze(dim=0)\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        return self._get_likelihood(imgs)\n",
    "\n",
    "    def encode(self, imgs):\n",
    "        # Given a batch of images, return the latent representation z and ldj of the transformations\n",
    "        z, ldj = imgs, torch.zeros(imgs.shape[0], device=self.device)\n",
    "        for flow in self.flows:\n",
    "            z, ldj = flow(z, ldj, reverse=False)\n",
    "        return z, ldj\n",
    "\n",
    "    def _get_likelihood(self, imgs, return_ll=False):\n",
    "        \"\"\"\n",
    "        Given a batch of images, return the likelihood of those.\n",
    "        If return_ll is True, this function returns the log likelihood of the input.\n",
    "        Otherwise, the ouptut metric is bits per dimension (scaled negative log likelihood)\n",
    "        \"\"\"\n",
    "        z, ldj = self.encode(imgs)\n",
    "        log_pz = self.prior.log_prob(z).sum(dim=[1,2,3])\n",
    "        log_px = ldj + log_pz\n",
    "        nll = -log_px\n",
    "        # Calculating bits per dimension\n",
    "        bpd = nll * np.log2(np.exp(1)) / np.prod(imgs.shape[1:])\n",
    "        return bpd.mean() if not return_ll else log_px\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, img_shape, z_init=None):\n",
    "        \"\"\"\n",
    "        Sample a batch of images from the flow.\n",
    "        \"\"\"\n",
    "        # Sample latent representation from prior\n",
    "        if z_init is None:\n",
    "            z = self.prior.sample(sample_shape=img_shape).to(device)\n",
    "        else:\n",
    "            z = z_init.to(device)\n",
    "\n",
    "        # Transform z to x by inverting the flows\n",
    "        ldj = torch.zeros(img_shape[0], device=device)\n",
    "        for flow in reversed(self.flows):\n",
    "            z, ldj = flow(z, ldj, reverse=True)\n",
    "        return z\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        # An scheduler is optional, but can help in flows to get the last bpd improvement\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.99)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Normalizing flows are trained by maximum likelihood => return bpd\n",
    "        loss = self._get_likelihood(batch[0])\n",
    "        self.log('train_bpd', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._get_likelihood(batch[0])\n",
    "        self.log('val_bpd', loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Perform importance sampling during testing => estimate likelihood M times for each image\n",
    "        samples = []\n",
    "        for _ in range(self.import_samples):\n",
    "            img_ll = self._get_likelihood(batch[0], return_ll=True)\n",
    "            samples.append(img_ll)\n",
    "        img_ll = torch.stack(samples, dim=-1)\n",
    "\n",
    "        # To average the probabilities, we need to go from log-space to exp, and back to log.\n",
    "        # Logsumexp provides us a stable implementation for this\n",
    "        img_ll = torch.logsumexp(img_ll, dim=-1) - np.log(self.import_samples)\n",
    "\n",
    "        # Calculate final bpd\n",
    "        bpd = -img_ll * np.log2(np.exp(1)) / np.prod(batch[0].shape[1:])\n",
    "        bpd = bpd.mean()\n",
    "\n",
    "        self.log('test_bpd', bpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df817ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dequantization(nn.Module):\n",
    "\n",
    "    def __init__(self, alpha=1e-5, quants=256):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            alpha - small constant that is used to scale the original input.\n",
    "                    Prevents dealing with values very close to 0 and 1 when inverting the sigmoid\n",
    "            quants - Number of possible discrete values (usually 256 for 8-bit image)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.quants = quants\n",
    "\n",
    "    def forward(self, z, ldj, reverse=False):\n",
    "        if not reverse:\n",
    "            z, ldj = self.dequant(z, ldj)\n",
    "            z, ldj = self.sigmoid(z, ldj, reverse=True)\n",
    "        else:\n",
    "            z, ldj = self.sigmoid(z, ldj, reverse=False)\n",
    "            z = z * self.quants\n",
    "            ldj += np.log(self.quants) * np.prod(z.shape[1:])\n",
    "            z = torch.floor(z).clamp(min=0, max=self.quants-1).to(torch.int32)\n",
    "        return z, ldj\n",
    "\n",
    "    def sigmoid(self, z, ldj, reverse=False):\n",
    "        # Applies an invertible sigmoid transformation\n",
    "        if not reverse:\n",
    "            ldj += (-z-2*F.softplus(-z)).sum(dim=[1,2,3])\n",
    "            z = torch.sigmoid(z)\n",
    "            # Reversing scaling for numerical stability\n",
    "            ldj -= np.log(1 - self.alpha) * np.prod(z.shape[1:])\n",
    "            z = (z - 0.5 * self.alpha) / (1 - self.alpha)\n",
    "        else:\n",
    "            z = z * (1 - self.alpha) + 0.5 * self.alpha  # Scale to prevent boundaries 0 and 1\n",
    "            ldj += np.log(1 - self.alpha) * np.prod(z.shape[1:])\n",
    "            ldj += (-torch.log(z) - torch.log(1-z)).sum(dim=[1,2,3])\n",
    "            z = torch.log(z) - torch.log(1-z)\n",
    "        return z, ldj\n",
    "\n",
    "    def dequant(self, z, ldj):\n",
    "        # Transform discrete values to continuous volumes\n",
    "        z = z.to(torch.float32)\n",
    "        z = z + torch.rand_like(z).detach()\n",
    "        z = z / self.quants\n",
    "        ldj -= np.log(self.quants) * np.prod(z.shape[1:])\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c94199a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully inverted dequantization\n"
     ]
    }
   ],
   "source": [
    "## Testing invertibility of dequantization layer\n",
    "pl.seed_everything(42)\n",
    "orig_img = train_set[0][0].unsqueeze(dim=0)\n",
    "ldj = torch.zeros(1,)\n",
    "dequant_module = Dequantization()\n",
    "deq_img, ldj = dequant_module(orig_img, ldj, reverse=False)\n",
    "reconst_img, ldj = dequant_module(deq_img, ldj, reverse=True)\n",
    "\n",
    "d1, d2 = torch.where(orig_img.squeeze() != reconst_img.squeeze())\n",
    "if len(d1) != 0:\n",
    "    print(\"Dequantization was not invertible.\")\n",
    "    for i in range(d1.shape[0]):\n",
    "        print(\"Original value:\", orig_img[0,0,d1[i], d2[i]].item())\n",
    "        print(\"Reconstructed value:\", reconst_img[0,0,d1[i], d2[i]].item())\n",
    "else:\n",
    "    print(\"Successfully inverted dequantization\")\n",
    "\n",
    "# Layer is not strictly invertible due to float precision constraints\n",
    "# assert (orig_img == reconst_img).all().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca750d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalDequantization(Dequantization):\n",
    "\n",
    "    def __init__(self, var_flows, alpha=1e-5):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            var_flows - A list of flow transformations to use for modeling q(u|x)\n",
    "            alpha - Small constant\n",
    "        \"\"\"\n",
    "        super().__init__(alpha=alpha)\n",
    "        self.flows = nn.ModuleList(var_flows)\n",
    "\n",
    "    def dequant(self, z, ldj):\n",
    "        z = z.to(torch.float32)\n",
    "        img = (z / 255.0) * 2 - 1 # We condition the flows on x, i.e. the original image\n",
    "\n",
    "        # Prior of u is a uniform distribution as before\n",
    "        # As most flow transformations are defined on [-infinity,+infinity], we apply an inverse sigmoid first.\n",
    "        deq_noise = torch.rand_like(z).detach()\n",
    "        deq_noise, ldj = self.sigmoid(deq_noise, ldj, reverse=True)\n",
    "        for flow in self.flows:\n",
    "            deq_noise, ldj = flow(deq_noise, ldj, reverse=False, orig_img=img)\n",
    "        deq_noise, ldj = self.sigmoid(deq_noise, ldj, reverse=False)\n",
    "\n",
    "        # After the flows, apply u as in standard dequantization\n",
    "        z = (z + deq_noise) / 256.0\n",
    "        ldj -= np.log(256.0) * np.prod(z.shape[1:])\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73fc72ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouplingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, network, mask, c_in):\n",
    "        \"\"\"\n",
    "        Coupling layer inside a normalizing flow.\n",
    "        Inputs:\n",
    "            network - A PyTorch nn.Module constituting the deep neural network for mu and sigma.\n",
    "                      Output shape should be twice the channel size as the input.\n",
    "            mask - Binary mask (0 or 1) where 0 denotes that the element should be transformed,\n",
    "                   while 1 means the latent will be used as input to the NN.\n",
    "            c_in - Number of input channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.network = network\n",
    "        self.scaling_factor = nn.Parameter(torch.zeros(c_in))\n",
    "        # Register mask as buffer as it is a tensor which is not a parameter,\n",
    "        # but should be part of the modules state.\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "    def forward(self, z, ldj, reverse=False, orig_img=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            z - Latent input to the flow\n",
    "            ldj - The current ldj of the previous flows.\n",
    "                  The ldj of this layer will be added to this tensor.\n",
    "            reverse - If True, we apply the inverse of the layer.\n",
    "            orig_img (optional) - Only needed in VarDeq. Allows external\n",
    "                                  input to condition the flow on (e.g. original image)\n",
    "        \"\"\"\n",
    "        # Apply network to masked input\n",
    "        z_in = z * self.mask\n",
    "        if orig_img is None:\n",
    "            nn_out = self.network(z_in)\n",
    "        else:\n",
    "            nn_out = self.network(torch.cat([z_in, orig_img], dim=1))\n",
    "        s, t = nn_out.chunk(2, dim=1)\n",
    "\n",
    "        # Stabilize scaling output\n",
    "        s_fac = self.scaling_factor.exp().view(1, -1, 1, 1)\n",
    "        s = torch.tanh(s / s_fac) * s_fac\n",
    "\n",
    "        # Mask outputs (only transform the second part)\n",
    "        s = s * (1 - self.mask)\n",
    "        t = t * (1 - self.mask)\n",
    "\n",
    "        # Affine transformation\n",
    "        if not reverse:\n",
    "            # Whether we first shift and then scale, or the other way round,\n",
    "            # is a design choice, and usually does not have a big impact\n",
    "            z = (z + t) * torch.exp(s)\n",
    "            ldj += s.sum(dim=[1,2,3])\n",
    "        else:\n",
    "            z = (z * torch.exp(-s)) - t\n",
    "            ldj -= s.sum(dim=[1,2,3])\n",
    "\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be9a246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkerboard_mask(h, w, invert=False):\n",
    "    x, y = torch.arange(h, dtype=torch.int32), torch.arange(w, dtype=torch.int32)\n",
    "    xx, yy = torch.meshgrid(x, y, indexing='ij')\n",
    "    mask = torch.fmod(xx + yy, 2)\n",
    "    mask = mask.to(torch.float32).view(1, 1, h, w)\n",
    "    if invert:\n",
    "        mask = 1 - mask\n",
    "    return mask\n",
    "\n",
    "def create_channel_mask(c_in, invert=False):\n",
    "    mask = torch.cat([torch.ones(c_in//2, dtype=torch.float32),\n",
    "                      torch.zeros(c_in-c_in//2, dtype=torch.float32)])\n",
    "    mask = mask.view(1, c_in, 1, 1)\n",
    "    if invert:\n",
    "        mask = 1 - mask\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62f3a75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Activation function that applies ELU in both direction (inverted and plain).\n",
    "    Allows non-linearity while providing strong gradients for any input (important for final convolution)\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([F.elu(x), F.elu(-x)], dim=1)\n",
    "\n",
    "\n",
    "class LayerNormChannels(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, eps=1e-5):\n",
    "        \"\"\"\n",
    "        This module applies layer norm across channels in an image.\n",
    "        Inputs:\n",
    "            c_in - Number of channels of the input\n",
    "            eps - Small constant to stabilize std\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(1, c_in, 1, 1))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, c_in, 1, 1))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=1, keepdim=True)\n",
    "        var = x.var(dim=1, unbiased=False, keepdim=True)\n",
    "        y = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        y = y * self.gamma + self.beta\n",
    "        return y\n",
    "\n",
    "\n",
    "class GatedConv(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_hidden):\n",
    "        \"\"\"\n",
    "        This module applies a two-layer convolutional ResNet block with input gate\n",
    "        Inputs:\n",
    "            c_in - Number of channels of the input\n",
    "            c_hidden - Number of hidden dimensions we want to model (usually similar to c_in)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            ConcatELU(),\n",
    "            nn.Conv2d(2*c_in, c_hidden, kernel_size=3, padding=1),\n",
    "            ConcatELU(),\n",
    "            nn.Conv2d(2*c_hidden, 2*c_in, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        val, gate = out.chunk(2, dim=1)\n",
    "        return x + val * torch.sigmoid(gate)\n",
    "\n",
    "\n",
    "class GatedConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_hidden=32, c_out=-1, num_layers=3):\n",
    "        \"\"\"\n",
    "        Module that summarizes the previous blocks to a full convolutional neural network.\n",
    "        Inputs:\n",
    "            c_in - Number of input channels\n",
    "            c_hidden - Number of hidden dimensions to use within the network\n",
    "            c_out - Number of output channels. If -1, 2 times the input channels are used (affine coupling)\n",
    "            num_layers - Number of gated ResNet blocks to apply\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_out = c_out if c_out > 0 else 2 * c_in\n",
    "        layers = []\n",
    "        layers += [nn.Conv2d(c_in, c_hidden, kernel_size=3, padding=1)]\n",
    "        for layer_index in range(num_layers):\n",
    "            layers += [GatedConv(c_hidden, c_hidden),\n",
    "                       LayerNormChannels(c_hidden)]\n",
    "        layers += [ConcatELU(),\n",
    "                   nn.Conv2d(2*c_hidden, c_out, kernel_size=3, padding=1)]\n",
    "        self.nn = nn.Sequential(*layers)\n",
    "\n",
    "        self.nn[-1].weight.data.zero_()\n",
    "        self.nn[-1].bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.nn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a1ef4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_flow(use_vardeq=True):\n",
    "    flow_layers = []\n",
    "    if use_vardeq:\n",
    "        vardeq_layers = [CouplingLayer(network=GatedConvNet(c_in=2, c_out=2, c_hidden=16),\n",
    "                                       mask=create_checkerboard_mask(h=28, w=28, invert=(i%2==1)),\n",
    "                                       c_in=1) for i in range(4)]\n",
    "        flow_layers += [VariationalDequantization(var_flows=vardeq_layers)]\n",
    "    else:\n",
    "        flow_layers += [Dequantization()]\n",
    "\n",
    "    for i in range(8):\n",
    "        flow_layers += [CouplingLayer(network=GatedConvNet(c_in=1, c_hidden=32),\n",
    "                                      mask=create_checkerboard_mask(h=28, w=28, invert=(i%2==1)),\n",
    "                                      c_in=1)]\n",
    "\n",
    "    flow_model = ImageFlow(flow_layers).to(device)\n",
    "    return flow_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed1cdd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_flow(flow, model_name=\"MNISTFlow\"):\n",
    "    # Create a PyTorch Lightning trainer\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, model_name),\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=200,\n",
    "                         gradient_clip_val=1.0,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"val_bpd\"),\n",
    "                                    LearningRateMonitor(\"epoch\")],\n",
    "                         check_val_every_n_epoch=5)\n",
    "    trainer.logger._log_graph = True\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    train_data_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=8)\n",
    "    result = None\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, model_name + \".ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        ckpt = torch.load(pretrained_filename, map_location=device)\n",
    "        flow.load_state_dict(ckpt['state_dict'])\n",
    "        result = ckpt.get(\"result\", None)\n",
    "    else:\n",
    "        print(\"Start training\", model_name)\n",
    "        trainer.fit(flow, train_data_loader, val_loader)\n",
    "\n",
    "    # Test best model on validation and test set if no result has been found\n",
    "    # Testing can be expensive due to the importance sampling.\n",
    "    if result is None:\n",
    "        val_result = trainer.test(flow, val_loader, verbose=False)\n",
    "        start_time = time.time()\n",
    "        test_result = trainer.test(flow, test_loader, verbose=False)\n",
    "        duration = time.time() - start_time\n",
    "        result = {\"test\": test_result, \"val\": val_result, \"time\": duration / len(test_loader) / flow.import_samples}\n",
    "\n",
    "    return flow, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25fc194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeFlow(nn.Module):\n",
    "\n",
    "    def forward(self, z, ldj, reverse=False):\n",
    "        B, C, H, W = z.shape\n",
    "        if not reverse:\n",
    "            # Forward direction: H x W x C => H/2 x W/2 x 4C\n",
    "            z = z.reshape(B, C, H//2, 2, W//2, 2)\n",
    "            z = z.permute(0, 1, 3, 5, 2, 4)\n",
    "            z = z.reshape(B, 4*C, H//2, W//2)\n",
    "        else:\n",
    "            # Reverse direction: H/2 x W/2 x 4C => H x W x C\n",
    "            z = z.reshape(B, C//4, 2, 2, H, W)\n",
    "            z = z.permute(0, 1, 4, 2, 5, 3)\n",
    "            z = z.reshape(B, C//4, H*2, W*2)\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21a57cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image (before)\n",
      " tensor([[[[ 1,  2,  3,  4],\n",
      "          [ 5,  6,  7,  8],\n",
      "          [ 9, 10, 11, 12],\n",
      "          [13, 14, 15, 16]]]])\n",
      "\n",
      "Image (forward)\n",
      " tensor([[[[ 1,  2,  5,  6],\n",
      "          [ 3,  4,  7,  8]],\n",
      "\n",
      "         [[ 9, 10, 13, 14],\n",
      "          [11, 12, 15, 16]]]])\n",
      "\n",
      "Image (reverse)\n",
      " tensor([[[[ 1,  2,  3,  4],\n",
      "          [ 5,  6,  7,  8],\n",
      "          [ 9, 10, 11, 12],\n",
      "          [13, 14, 15, 16]]]])\n"
     ]
    }
   ],
   "source": [
    "sq_flow = SqueezeFlow()\n",
    "rand_img = torch.arange(1,17).view(1, 1, 4, 4)\n",
    "print(\"Image (before)\\n\", rand_img)\n",
    "forward_img, _ = sq_flow(rand_img, ldj=None, reverse=False)\n",
    "print(\"\\nImage (forward)\\n\", forward_img.permute(0,2,3,1)) # Permute for readability\n",
    "reconst_img, _ = sq_flow(forward_img, ldj=None, reverse=True)\n",
    "print(\"\\nImage (reverse)\\n\", reconst_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dd368d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitFlow(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prior = torch.distributions.normal.Normal(loc=0.0, scale=1.0)\n",
    "\n",
    "    def forward(self, z, ldj, reverse=False):\n",
    "        if not reverse:\n",
    "            z, z_split = z.chunk(2, dim=1)\n",
    "            ldj += self.prior.log_prob(z_split).sum(dim=[1,2,3])\n",
    "        else:\n",
    "            z_split = self.prior.sample(sample_shape=z.shape).to(device)\n",
    "            z = torch.cat([z, z_split], dim=1)\n",
    "            ldj -= self.prior.log_prob(z_split).sum(dim=[1,2,3])\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f664a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiscale_flow():\n",
    "    flow_layers = []\n",
    "\n",
    "    vardeq_layers = [CouplingLayer(network=GatedConvNet(c_in=2, c_out=2, c_hidden=16),\n",
    "                                   mask=create_checkerboard_mask(h=28, w=28, invert=(i%2==1)),\n",
    "                                   c_in=1) for i in range(4)]\n",
    "    flow_layers += [VariationalDequantization(vardeq_layers)]\n",
    "\n",
    "    flow_layers += [CouplingLayer(network=GatedConvNet(c_in=1, c_hidden=32),\n",
    "                                  mask=create_checkerboard_mask(h=28, w=28, invert=(i%2==1)),\n",
    "                                  c_in=1) for i in range(2)]\n",
    "    flow_layers += [SqueezeFlow()]\n",
    "    for i in range(2):\n",
    "        flow_layers += [CouplingLayer(network=GatedConvNet(c_in=4, c_hidden=48),\n",
    "                                      mask=create_channel_mask(c_in=4, invert=(i%2==1)),\n",
    "                                      c_in=4)]\n",
    "    flow_layers += [SplitFlow(),\n",
    "                    SqueezeFlow()]\n",
    "    for i in range(4):\n",
    "        flow_layers += [CouplingLayer(network=GatedConvNet(c_in=8, c_hidden=64),\n",
    "                                      mask=create_channel_mask(c_in=8, invert=(i%2==1)),\n",
    "                                      c_in=8)]\n",
    "\n",
    "    flow_model = ImageFlow(flow_layers).to(device)\n",
    "    return flow_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fde954cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 556,312\n",
      "Number of parameters: 628,388\n",
      "Number of parameters: 1,711,818\n"
     ]
    }
   ],
   "source": [
    "def print_num_params(model):\n",
    "    num_params = sum([np.prod(p.shape) for p in model.parameters()])\n",
    "    print(\"Number of parameters: {:,}\".format(num_params))\n",
    "\n",
    "print_num_params(create_simple_flow(use_vardeq=False))\n",
    "print_num_params(create_simple_flow(use_vardeq=True))\n",
    "print_num_params(create_multiscale_flow())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87d38861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pretrained model, loading...\n",
      "Found pretrained model, loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pretrained model, loading...\n"
     ]
    }
   ],
   "source": [
    "flow_dict = {\"simple\": {}, \"vardeq\": {}, \"multiscale\": {}}\n",
    "flow_dict[\"simple\"][\"model\"], flow_dict[\"simple\"][\"result\"] = train_flow(create_simple_flow(use_vardeq=False), model_name=\"MNISTFlow_simple\")\n",
    "flow_dict[\"vardeq\"][\"model\"], flow_dict[\"vardeq\"][\"result\"] = train_flow(create_simple_flow(use_vardeq=True), model_name=\"MNISTFlow_vardeq\")\n",
    "flow_dict[\"multiscale\"][\"model\"], flow_dict[\"multiscale\"][\"result\"] = train_flow(create_multiscale_flow(), model_name=\"MNISTFlow_multiscale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5601eb2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': [{'test_bpd': 1.07839834690094}],\n",
       " 'val': [{'test_bpd': 1.0798484086990356}],\n",
       " 'time': 0.019570968523147,\n",
       " 'samp_time': 0.017716965675354003}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow_dict[\"simple\"][\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "defaf244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Model     </th><th>Validation Bpd  </th><th>Test Bpd  </th><th>Inference time  </th><th>Sampling time  </th><th>Num Parameters  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>simple    </td><td>1.080 bpd       </td><td>1.078 bpd </td><td>20 ms           </td><td>18 ms          </td><td>556,312         </td></tr>\n",
       "<tr><td>vardeq    </td><td>1.045 bpd       </td><td>1.043 bpd </td><td>26 ms           </td><td>18 ms          </td><td>628,388         </td></tr>\n",
       "<tr><td>multiscale</td><td>1.022 bpd       </td><td>1.020 bpd </td><td>23 ms           </td><td>15 ms          </td><td>1,711,818       </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tabulate\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "table = [[key,\n",
    "          \"%4.3f bpd\" % flow_dict[key][\"result\"][\"val\"][0][\"test_bpd\"],\n",
    "          \"%4.3f bpd\" % flow_dict[key][\"result\"][\"test\"][0][\"test_bpd\"],\n",
    "          \"%2.0f ms\" % (1000 * flow_dict[key][\"result\"][\"time\"]),\n",
    "          \"%2.0f ms\" % (1000 * flow_dict[key][\"result\"].get(\"samp_time\", 0)),\n",
    "          \"{:,}\".format(sum([np.prod(p.shape) for p in flow_dict[key][\"model\"].parameters()]))]\n",
    "         for key in flow_dict]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html', headers=[\"Model\", \"Validation Bpd\", \"Test Bpd\", \"Inference time\", \"Sampling time\", \"Num Parameters\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18353d74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
